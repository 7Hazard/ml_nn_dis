{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObJ5q2mTlr_J",
        "outputId": "8487aa79-d656-4a6d-8100-71c44a930503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/karolzak/keras-unet\n",
            "  Cloning https://github.com/karolzak/keras-unet to c:\\users\\hazard\\appdata\\local\\temp\\pip-req-build-k08afn50\n",
            "Building wheels for collected packages: keras-unet\n",
            "  Building wheel for keras-unet (setup.py): started\n",
            "  Building wheel for keras-unet (setup.py): finished with status 'done'\n",
            "  Created wheel for keras-unet: filename=keras_unet-0.1.2-py3-none-any.whl size=17139 sha256=3e1b38c30c047356a6b4bc5a2a1adccb06d9a8b957d5834e37bfaab5b010ad89\n",
            "  Stored in directory: C:\\Users\\Hazard\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-l3vtxrs8\\wheels\\df\\f6\\e3\\829702eaa69ce1a9a8db64c6f49e2b7f2bd5ef8e534ba08070\n",
            "Successfully built keras-unet\n",
            "Installing collected packages: keras-unet\n",
            "Successfully installed keras-unet-0.1.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 20.2.1; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/karolzak/keras-unet\n",
        "# ! pip install git+https://github.com/qubvel/segmentation_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2sozMztGrdJ"
      },
      "source": [
        "In the directory \"raw\", there exists more images that there are pairs with masks. We realize that there is a difference between the images which names start with TXXXH and TXXXO. The TXXXH seemingly contain more noise than the TXXXO, so we could assume that those are preprocessed by denoising in some way.\n",
        "\n",
        "We preprocess all the images and downsize them from 1400x1400 to 400x400, also convert the images to grayscale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "_lEnMvgiGqqT",
        "outputId": "e651124b-a547-4054-8758-b240b324651f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAAmklEQVR4nO2S0Q6AIAhF1f//Z3ppq8m9Ckpubdy3Ak8HrJY7Usyp74cG39rPPwAHgQDWBBYA3MA2Q98VaXAGoOZso+IXBrsALRlrsLIEnwH4ws9+JLSjNu2INNgFQL9oA/8SHAYYHr5EPgOpnLtGpqYA3nuwGlCuBjgVgAEicCoaQXcPrOAO+v7RVKwmhp5JUQynM5lMJpPJqFyCgQNi6nlg7wAAAABJRU5ErkJggg==",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os, re\n",
        "from PIL import Image\n",
        "from IPython.display import Image as Img, display\n",
        "\n",
        "img_size = 64, 64\n",
        "input_pattern = re.compile(\"T[0-9]{3}O_crop_[0-9].png\")\n",
        "input_dir = \"raw/\"\n",
        "target_pattern = re.compile(\"T[0-9]{3}OM_crop_[0-9].png\")\n",
        "target_dir = \"mask/\"\n",
        "\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_dir, fname)\n",
        "        for fname in os.listdir(target_dir)\n",
        "        if target_pattern.match(fname)\n",
        "    ]\n",
        ")\n",
        "input_img_paths = [\"\"]\n",
        "input_img_paths.remove(\"\") # for autocomplete\n",
        "for fname in target_img_paths:\n",
        "  fname = fname.replace(\"M\", \"\").replace(\"mask/\", \"raw/\")\n",
        "  if os.path.isfile(fname):\n",
        "    input_img_paths.append(fname)\n",
        "  else:\n",
        "    print(\"Bad input file: \", fname)\n",
        "\n",
        "if not os.path.exists(\"target\"):\n",
        "    os.mkdir(\"target\")\n",
        "for infile in target_img_paths:\n",
        "    outfile = infile.replace(\"mask/\", \"target/\")\n",
        "    if infile != outfile:\n",
        "        try:\n",
        "            im = Image.open(infile)\n",
        "            # Downscale without smoothing/antialiasing with NEAREST alg\n",
        "            # to get exactly either 0 or 255 pixel value\n",
        "            im.thumbnail(img_size, Image.NEAREST)\n",
        "            im.save(outfile, \"PNG\")\n",
        "        except IOError:\n",
        "            print(\"cannot create thumbnail for '%s'\" % infile)\n",
        "\n",
        "# Show example of produced downscaled image\n",
        "display(Img(filename=target_img_paths[9].replace(\"mask/\", \"target/\")))\n",
        "\n",
        "if not os.path.exists(\"input\"):\n",
        "    os.mkdir(\"input\")\n",
        "for infile in input_img_paths:\n",
        "    outfile = infile.replace(\"raw/\", \"input/\")\n",
        "    if infile != outfile:\n",
        "        try:\n",
        "            im = Image.open(infile)\n",
        "            \n",
        "            # to grayscale\n",
        "            # im = im.convert('L')\n",
        "\n",
        "            im.thumbnail(img_size)\n",
        "            im.save(outfile, \"PNG\")\n",
        "        except IOError:\n",
        "            print(\"cannot create thumbnail for '%s'\" % infile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "683QcTopltlc"
      },
      "source": [
        "We read all the files based on the existance of its accompanying masked equivalent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GjWxA7ulpmP",
        "outputId": "68051fdc-b6a7-46db-dfdd-8fd30e18fb73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples: 153 153\n",
            "input/T001O_crop_0.png | target/T001OM_crop_0.png\n",
            "input/T001O_crop_1.png | target/T001OM_crop_1.png\n",
            "input/T002O_crop_2.png | target/T002OM_crop_2.png\n",
            "input/T002O_crop_3.png | target/T002OM_crop_3.png\n",
            "input/T002O_crop_4.png | target/T002OM_crop_4.png\n",
            "input/T003O_crop_1.png | target/T003OM_crop_1.png\n",
            "input/T003O_crop_2.png | target/T003OM_crop_2.png\n",
            "input/T003O_crop_3.png | target/T003OM_crop_3.png\n",
            "input/T003O_crop_4.png | target/T003OM_crop_4.png\n",
            "input/T004O_crop_1.png | target/T004OM_crop_1.png\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "input_dir = \"input/\"\n",
        "target_dir = \"target/\"\n",
        "\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_dir, fname)\n",
        "        for fname in os.listdir(target_dir)\n",
        "    ]\n",
        ")\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_dir, fname)\n",
        "        for fname in os.listdir(input_dir)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Number of samples:\", len(input_img_paths), len(target_img_paths))\n",
        "\n",
        "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "    print(input_path, \"|\", target_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLTTW8ZB0BcV"
      },
      "source": [
        "We evaluate whether the raw image is matched with the correct mask image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "503IcMTL87CM",
        "outputId": "abcfefd4-015c-4acc-aada-83da359bd813"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64 at 0x1E351A7D700>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAA00lEQVR4nO2ZSxaEIAwExfvfmbmAjPlh+mnVVgUqnbBxHAvmnKtHzzDGsLx2Jr/fhH33pYBrlUb+CXThKpyigIsbgee7yLvj2xPQR0sg0LH3AuKXqVYCAYQEYlELCcQwCSiPgUoC4RqpCIRBoILMjFkFZOdYIoEM/QLJbPsFkjgENMegOYF8Ub7UQkd1F5Ws9rEECqkK0y2gdhf1JFBYhYiAVAjBBDIOtf7xFoqdozy91Ax4T7Oj92pWtPzO2TQ5lYteakhNPAAAAAAAAAAAAMBL+AHy2AmaR+pc5AAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x1E35135E9A0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from keras.preprocessing.image import load_img\n",
        "from PIL import ImageOps\n",
        "import numpy as np\n",
        "\n",
        "# Display input image #9\n",
        "raw = load_img(input_img_paths[9])\n",
        "display(raw)\n",
        "\n",
        "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
        "# mask = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
        "mask = load_img(target_img_paths[9])\n",
        "display(ImageOps.autocontrast(mask))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sDn-CZ5ED5GH",
        "outputId": "38bc8dbd-2b56-460d-8046-cb753ee64f5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "8/8 [==============================] - ETA: 0s - loss: 332.4624 - accuracy: 0.0653"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAA7ElEQVR4nO2ZQRKEMAgEk/3/n7MHT+saAoGYUaaPlkXSDBwsa/mltVZQqbX+P/xoXkKgd7GzgPAqJhcCBc9BuM+1ABRyNx8gINMVQJuiHugJDPuILjBEEtg+RZoLvDqBR4AroBzggcD2NRiCm4ASUAF98qACesYC4GuAmICpZYgCJnII3LkG1rNyJICMVuCeKZo4JU0CsBgEVk/RXP1MCZSVIUxXTpYAIGaBFVPkqZkvgQL2iTOZQKCDs1TKEToICcFfxJUAwjJ4R8jjEOIfsAN7c4g82/STPEo7vnmyBsLaEEIIIYQQQgghBIEv+2YJugs2zWMAAAAASUVORK5CYII=",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x1E30E6C2880>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAAG0lEQVR4nO3BgQAAAADDoPlT3+AEVQEAAAB8AxBAAAFGOA0dAAAAAElFTkSuQmCC",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=64x64 at 0x1E3059F8280>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample Prediction after binary_crossentropy rmsprop epoch 1\n",
            "\n",
            "8/8 [==============================] - 23s 3s/step - loss: 332.4624 - accuracy: 0.0653\n",
            "Epoch 2/4\n",
            "8/8 [==============================] - ETA: 0s - loss: 2.5124 - accuracy: 0.0653"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAA7ElEQVR4nO2ZQRKEMAgEk/3/n7MHT+saAoGYUaaPlkXSDBwsa/mltVZQqbX+P/xoXkKgd7GzgPAqJhcCBc9BuM+1ABRyNx8gINMVQJuiHugJDPuILjBEEtg+RZoLvDqBR4AroBzggcD2NRiCm4ASUAF98qACesYC4GuAmICpZYgCJnII3LkG1rNyJICMVuCeKZo4JU0CsBgEVk/RXP1MCZSVIUxXTpYAIGaBFVPkqZkvgQL2iTOZQKCDs1TKEToICcFfxJUAwjJ4R8jjEOIfsAN7c4g82/STPEo7vnmyBsLaEEIIIYQQQgghBIEv+2YJugs2zWMAAAAASUVORK5CYII=",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x1E30CDBD3D0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAAG0lEQVR4nO3BgQAAAADDoPlT3+AEVQEAAAB8AxBAAAFGOA0dAAAAAElFTkSuQmCC",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=64x64 at 0x1E30CDE3D90>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample Prediction after binary_crossentropy rmsprop epoch 2\n",
            "\n",
            "8/8 [==============================] - 21s 3s/step - loss: 2.5124 - accuracy: 0.0653\n",
            "Epoch 3/4\n",
            "6/8 [=====================>........] - ETA: 5s - loss: 1.4940 - accuracy: 0.0460"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\src\\KTH\\ml_nn_dis\\doppler_image_segmentation.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/src/KTH/ml_nn_dis/doppler_image_segmentation.ipynb#ch0000008?line=177'>178</a>\u001b[0m   model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39mloss, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/src/KTH/ml_nn_dis/doppler_image_segmentation.ipynb#ch0000008?line=178'>179</a>\u001b[0m   model\u001b[39m.\u001b[39mfit(train_gen, batch_size\u001b[39m=\u001b[39mbatch_size, epochs\u001b[39m=\u001b[39mepochs, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/src/KTH/ml_nn_dis/doppler_image_segmentation.ipynb#ch0000008?line=180'>181</a>\u001b[0m train(\u001b[39m\"\u001b[39;49m\u001b[39mbinary_crossentropy\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrmsprop\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "\u001b[1;32md:\\src\\KTH\\ml_nn_dis\\doppler_image_segmentation.ipynb Cell 8'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(loss, optimizer)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/src/KTH/ml_nn_dis/doppler_image_segmentation.ipynb#ch0000008?line=175'>176</a>\u001b[0m callbacks \u001b[39m=\u001b[39m [DisplayCallback(model, loss, optimizer)]\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/src/KTH/ml_nn_dis/doppler_image_segmentation.ipynb#ch0000008?line=177'>178</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39mloss, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/src/KTH/ml_nn_dis/doppler_image_segmentation.ipynb#ch0000008?line=178'>179</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_gen, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mepochs, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n",
            "File \u001b[1;32mC:\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Python38/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Python38/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Python38/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Python38/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Python38/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mC:\\Python38\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> <a href='file:///c%3A/Python38/lib/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mC:\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mC:\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mC:\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32mC:\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[1;32mC:\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[0;32m   <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32mC:\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32mC:\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///c%3A/Python38/lib/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import load_img\n",
        "\n",
        "class TrainSequence(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        \n",
        "        # 3 channels (rgb)\n",
        "        input = np.zeros((self.batch_size, self.img_size[0], self.img_size[1], 3), dtype='float32')\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            # img = load_img(path, target_size=self.img_size, color_mode='grayscale')\n",
        "            img = load_img(path, target_size=self.img_size)\n",
        "            img_arr = np.array(img)\n",
        "            # img_arr = np.expand_dims(img_arr, axis=-1)\n",
        "            input[j] = img_arr\n",
        "\n",
        "        # 3 channels (rgb)\n",
        "        target = np.zeros((self.batch_size, self.img_size[0], self.img_size[1], 1), dtype='float32')\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size, color_mode='grayscale')\n",
        "            # img = load_img(path, target_size=self.img_size)\n",
        "            img_arr = np.array(img)\n",
        "            for img_x, row in enumerate(img_arr):\n",
        "              for img_y, pixel in enumerate(row):\n",
        "                img_arr[img_x][img_y] = 1 if (pixel == 255) else 0\n",
        "            img_arr = np.expand_dims(img_arr, axis=-1)\n",
        "            target[j] = img_arr\n",
        "\n",
        "        return input, target\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "# Split our img paths into a training and a validation set\n",
        "val_samples = 30\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "train_target_img_paths = target_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "num_classes = 2\n",
        "batch_size = 14 # we get 13 in the last batch, good (153 mod 14 = 13)\n",
        "# see https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
        "train_gen = TrainSequence(\n",
        "    batch_size, \n",
        "    img_size, \n",
        "    train_input_img_paths,\n",
        "    val_target_img_paths)\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import load_img\n",
        "from IPython.display import Image, display\n",
        "from PIL import ImageOps\n",
        "\n",
        "# Configure the model for training.\n",
        "# We use the \"sparse\" version of categorical_crossentropy\n",
        "# because our target data is integers.\n",
        "\n",
        "epochs = 4\n",
        "\n",
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, model, loss, optimizer):\n",
        "    self.model = model\n",
        "    self.loss = loss\n",
        "    self.optimizer = optimizer\n",
        "  \n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    pred_mask = self.model.predict(TrainSequence(1, img_size, [input_img_paths[9]], [target_img_paths[9]]))[0]\n",
        "    # pred_mask = np.argmax(pred_mask, axis=-1)\n",
        "    # pred_mask = np.expand_dims(pred_mask, axis=-1)\n",
        "    pred_mask = keras.preprocessing.image.array_to_img(pred_mask)\n",
        "    # display(load_img(input_img_paths[9]))\n",
        "    display(load_img(target_img_paths[9]))\n",
        "    display(pred_mask)\n",
        "    print(f'\\nSample Prediction after {self.loss} {self.optimizer} epoch {epoch+1}\\n')\n",
        "\n",
        "import segmentation_models as sm\n",
        "\n",
        "def train(loss, optimizer):\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  model = sm.Unet(\n",
        "    'resnet34', classes=1, activation='sigmoid', encoder_weights=None)\n",
        "\n",
        "  callbacks = [DisplayCallback(model, loss, optimizer)]\n",
        "  \n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  model.fit(train_gen, batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "train(\"binary_crossentropy\", \"rmsprop\")\n",
        "# train(\"binary_crossentropy\", \"adam\")\n",
        "# train(\"sparse_categorical_crossentropy\", \"rmsprop\")\n",
        "# train(\"sparse_categorical_crossentropy\", \"adam\")\n",
        "# train(\"categorical_crossentropy\", \"rmsprop\")\n",
        "# train(\"categorical_crossentropy\", \"adam\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "doppler image segmentation",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
